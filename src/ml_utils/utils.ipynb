{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utility Functions for Deep Learning Workflows\n",
        "\n",
        "This notebook contains utility functions and examples for common deep learning tasks in PyTorch. It demonstrates:\n",
        "\n",
        "# Table of Contents\n",
        "1. [Kernel Connection Information](#kernel-connection-information)\n",
        "2. [Module Auto-reloading Setup](#module-auto-reloading-setup)  \n",
        "3. [Example Usage: Data Loading and Preprocessing](#example-usage-data-loading-and-preprocessing)\n",
        "\n",
        "\n",
        "The following cells provide practical examples of these utilities in action.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added d:\\Development\\Python\\PyTorch to Python path\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "\n",
        "\"\"\"Only needed for this util notebook cuz it's in the utils folder\"\"\"\n",
        "\"\"\"Add the project root directory to Python path.\"\"\"\n",
        "# Get the absolute path of this file's directory\n",
        "# In Jupyter notebooks, __file__ is not defined\n",
        "# Use notebook's directory instead\n",
        "try:\n",
        "    # Try to get the notebook's directory\n",
        "    current_dir = os.getcwd()\n",
        "except:\n",
        "    # Fallback if that doesn't work\n",
        "    current_dir = os.path.abspath('.')\n",
        "# Get the parent directory (project root)\n",
        "project_root = os.path.dirname(current_dir)\n",
        "# Add to Python path if not already there\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "    print(f\"Added {project_root} to Python path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel Connection Information\n",
        "\n",
        "The following code cell retrieves and displays information about the current Jupyter kernel connection. It:\n",
        "\n",
        "1. Accesses the IPython kernel configuration\n",
        "2. Reads the connection file to extract port information\n",
        "3. Prints the port number where the kernel is running\n",
        "\n",
        "This is useful for debugging connection issues or when you need to connect to the kernel programmatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kernel is running on port: 57954\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from IPython.core.getipython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "if ipython is not None and hasattr(ipython.config, 'IPKernelApp'):\n",
        "    connection_file = ipython.config['IPKernelApp']['connection_file']\n",
        "    with open(connection_file) as f:\n",
        "        config = json.load(f)\n",
        "    print(f\"Kernel is running on port: {config['shell_port']}\")\n",
        "else:\n",
        "    print(\"Not running in an IPython kernel or kernel information not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module Auto-reloading Setup\n",
        "\n",
        "The following code cell demonstrates how to set up module auto-reloading in Jupyter. It:\n",
        "\n",
        "1. Imports necessary modules from the utils package\n",
        "2. Enables the autoreload extension with `%load_ext autoreload`\n",
        "3. Configures it to automatically reload all modules before executing code with `%autoreload 2`\n",
        "\n",
        "This is particularly useful during development when you're frequently making changes to imported modules and want those changes to be reflected without restarting the kernel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autoreload enabled: Changes to imported modules will be reloaded automatically\n"
          ]
        }
      ],
      "source": [
        "from ml_utils import get_dataloaders, torch, TensorDataset\n",
        "\n",
        "# Enable autoreload extension\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# This will automatically reload modules before executing code\n",
        "# Useful during development when you're frequently changing code in imported modules\n",
        "print(\"Autoreload enabled: Changes to imported modules will be reloaded automatically\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Usage: Data Loading and Preprocessing\n",
        "\n",
        "This example demonstrates how to use the `get_preprocessed_dataloaders` function to create data loaders with automatic preprocessing. It shows:\n",
        "\n",
        "- Creating synthetic training and validation datasets\n",
        "- Defining a preprocessing function for data normalization \n",
        "- Setting up DataLoaders with automatic preprocessing\n",
        "- Inspecting the resulting batch sizes and data shapes\n",
        "\n",
        "The code illustrates proper usage of the data loading utilities with preprocessing capabilities.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of batches in training loader: 4\n",
            "Number of batches in validation loader: 1\n",
            "Batch shape: X=torch.Size([32, 5]), y=torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Example script demonstrating how to use get_dataloaders properly\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create some sample data\n",
        "# For example, 100 samples with 5 features\n",
        "from ml_utils import get_preprocessed_dataloaders, torch, TensorDataset\n",
        "\n",
        "x_train = torch.randn(100, 5)\n",
        "y_train = torch.randint(0, 2, (100,))\n",
        "\n",
        "x_valid = torch.randn(20, 5)\n",
        "y_valid = torch.randint(0, 2, (20,))\n",
        "\n",
        "# Create TensorDataset objects\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "valid_ds = TensorDataset(x_valid, y_valid)\n",
        "# Define a preprocessing function to normalize the data\n",
        "def preprocess(x, y):\n",
        "    # Normalize the input data to have zero mean and unit variance\n",
        "    mean = x.mean(0, keepdim=True)\n",
        "    std = x.std(0, keepdim=True) + 1e-7  # Add small epsilon to avoid division by zero\n",
        "    x_normalized = (x - mean) / std\n",
        "    return x_normalized, y\n",
        "\n",
        "# Create preprocessed dataloaders with batch size of 32\n",
        "train_loader, valid_loader = get_preprocessed_dataloaders(\n",
        "    train_ds=train_ds,\n",
        "    valid_ds=valid_ds,\n",
        "    bs=32,\n",
        "    preprocess=preprocess\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Number of batches in training loader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in validation loader: {len(valid_loader)}\")\n",
        "\n",
        "# Get one batch from the training loader\n",
        "for X_batch, y_batch in train_loader:\n",
        "    print(f\"Batch shape: X={X_batch.shape}, y={y_batch.shape}\")\n",
        "    break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 11.2M/26.4M [00:46<01:02, 243kB/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 44\u001b[0m\n\u001b[0;32m     38\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     39\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     40\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))\n\u001b[0;32m     41\u001b[0m ])\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Download training data\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m training_set \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFashionMNIST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Create data loader\u001b[39;00m\n\u001b[0;32m     49\u001b[0m training_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m     50\u001b[0m     training_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     51\u001b[0m )\n",
            "File \u001b[1;32md:\\Work\\Anaconda\\envs\\HFRL\\lib\\site-packages\\torchvision\\datasets\\mnist.py:100\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\Work\\Anaconda\\envs\\HFRL\\lib\\site-packages\\torchvision\\datasets\\mnist.py:188\u001b[0m, in \u001b[0;36mMNIST.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmirror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    190\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(e)\n",
            "File \u001b[1;32md:\\Work\\Anaconda\\envs\\HFRL\\lib\\site-packages\\torchvision\\datasets\\utils.py:391\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    389\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 391\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    394\u001b[0m extract_archive(archive, extract_root, remove_finished)\n",
            "File \u001b[1;32md:\\Work\\Anaconda\\envs\\HFRL\\lib\\site-packages\\torchvision\\datasets\\utils.py:130\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32md:\\Work\\Anaconda\\envs\\HFRL\\lib\\site-packages\\torchvision\\datasets\\utils.py:30\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 30\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     31\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[0;32m     32\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
            "File \u001b[1;32md:\\Work\\Anaconda\\envs\\HFRL\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
            "File \u001b[1;32md:\\Work\\Anaconda\\envs\\HFRL\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from ml_utils import (\n",
        "    torch, \n",
        "    torchvision, \n",
        "    transforms,     \n",
        "    get_tensorboard_writer,\n",
        "    add_images_to_tensorboard,\n",
        "    matplotlib_imshow,\n",
        "    log_model_graph,\n",
        "    log_scalars,\n",
        "    close_writer\n",
        ")\n",
        "\n",
        "# Sample model for demonstration\n",
        "class SimpleModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
        "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = torch.nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = torch.nn.Linear(120, 84)\n",
        "        self.fc3 = torch.nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Create a TensorBoard writer\n",
        "writer = get_tensorboard_writer('runs/visualization_example')\n",
        "\n",
        "# Load some example data (Fashion MNIST)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "    \n",
        "    # Download training data\n",
        "training_set = torchvision.datasets.FashionMNIST(\n",
        "    '../data', download=True, train=True, transform=transform\n",
        ")\n",
        "    \n",
        "# Create data loader\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    training_set, batch_size=4, shuffle=True, num_workers=2\n",
        ")\n",
        "    \n",
        "# Get a batch of training data\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Add images to TensorBoard\n",
        "add_images_to_tensorboard(writer, images, 'Fashion-MNIST Samples')\n",
        "    \n",
        "# Initialize model\n",
        "model = SimpleModel()\n",
        "    \n",
        "    # Log model graph\n",
        "log_model_graph(writer, model, images)\n",
        "    \n",
        "# Log some dummy training metrics\n",
        "for epoch in range(5):\n",
        "    # Simulate training and validation losses\n",
        "    train_loss = 1.0 / (epoch + 1)\n",
        "    val_loss = 1.2 / (epoch + 1)\n",
        "    \n",
        "    # Log metrics\n",
        "    log_scalars(\n",
        "        writer, \n",
        "        'Training vs. Validation Loss',\n",
        "        {'Training': train_loss, 'Validation': val_loss},\n",
        "        epoch\n",
        "    )\n",
        "\n",
        "# Close the writer\n",
        "close_writer(writer)\n",
        "\n",
        "print(\"TensorBoard data has been logged to 'runs/visualization_example'\")\n",
        "print(\"To view it, run: tensorboard --logdir=runs\")\n",
        "print(\"Then open http://localhost:6006/ in your browser\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "HFRL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
